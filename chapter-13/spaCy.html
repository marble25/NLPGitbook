
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>spaCy와 딥러닝 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    
    <link rel="prev" href="keras.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    프로젝트 소개
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 1 - Text Analysis</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../chapter-1/what_is_text_analysis.html">
            
                <a href="../chapter-1/what_is_text_analysis.html">
            
                    
                    Text Analysis란?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../chapter-1/importance_of_input.html">
            
                <a href="../chapter-1/importance_of_input.html">
            
                    
                    Garbage in, Garbage out
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../chapter-1/importance_of_text_analysis.html">
            
                <a href="../chapter-1/importance_of_text_analysis.html">
            
                    
                    Text Analysis를 시작하자!
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="../chapter-1/why_python.html">
            
                <a href="../chapter-1/why_python.html">
            
                    
                    Python?
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 2 - Language Model</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../chatper-2/spaCy.md">
            
                <span>
            
                    
                    spaCy
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../chapter-2/install_lm.html">
            
                <a href="../chapter-2/install_lm.html">
            
                    
                    Language Model 설치
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="../chapter-2/tokenize_text.html">
            
                <a href="../chapter-2/tokenize_text.html">
            
                    
                    텍스트 토큰화
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.4" data-path="../chapter-2/pos_tagging.html">
            
                <a href="../chapter-2/pos_tagging.html">
            
                    
                    POS 태깅
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="../chapter-2/ner_tagging.html">
            
                <a href="../chapter-2/ner_tagging.html">
            
                    
                    NER 태깅
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="../chapter-2/preprocessing.html">
            
                <a href="../chapter-2/preprocessing.html">
            
                    
                    텍스트 전처리
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 3 - Text를 Vector로</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="../chapter-3/gensim.html">
            
                <a href="../chapter-3/gensim.html">
            
                    
                    Gensim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="../chapter-3/bow.html">
            
                <a href="../chapter-3/bow.html">
            
                    
                    Bag-of-words
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="../chapter-3/tf_idf.html">
            
                <a href="../chapter-3/tf_idf.html">
            
                    
                    TF-IDF
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="../chapter-3/vector_transformation_in_gensim.html">
            
                <a href="../chapter-3/vector_transformation_in_gensim.html">
            
                    
                    Gensim에서 벡터 표현법
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="../chapter-3/n_gram.html">
            
                <a href="../chapter-3/n_gram.html">
            
                    
                    N-gram과 전처리
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chpater 4 - POS Tagging</li>
        
        
    
        <li class="chapter " data-level="5.1" data-path="../chapter-4/pos_tagging.html">
            
                <a href="../chapter-4/pos_tagging.html">
            
                    
                    POS Tagging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.2" data-path="../chapter-4/python_pos_tagging.html">
            
                <a href="../chapter-4/python_pos_tagging.html">
            
                    
                    Python에서 POS Tagging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.3" data-path="../chapter-4/training_pos_tagger.html">
            
                <a href="../chapter-4/training_pos_tagger.html">
            
                    
                    POS Tagger 학습시키기
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.4" data-path="../chapter-4/pos_tagging_code.html">
            
                <a href="../chapter-4/pos_tagging_code.html">
            
                    
                    POS Tagging Code
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 5 - NER Tagging</li>
        
        
    
        <li class="chapter " data-level="6.1" data-path="../chapter-5/ner_tagging.html">
            
                <a href="../chapter-5/ner_tagging.html">
            
                    
                    NER Tagging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.2" data-path="../chapter-5/python_ner_tagging.html">
            
                <a href="../chapter-5/python_ner_tagging.html">
            
                    
                    Python에서 NER Tagging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.3" data-path="../chapter-5/training_ner_tagger.html">
            
                <a href="../chapter-5/training_ner_tagger.html">
            
                    
                    NER Tagger 학습시키기
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.4" data-path="../chapter-5/ner_tagging_code.html">
            
                <a href="../chapter-5/ner_tagging_code.html">
            
                    
                    NER Tagging Code
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 6 - Dependency Parsing</li>
        
        
    
        <li class="chapter " data-level="7.1" data-path="../chapter-6/dependency_parsing.html">
            
                <a href="../chapter-6/dependency_parsing.html">
            
                    
                    Dependency Parsing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.2" data-path="../chapter-6/python_dependency_parsing.html">
            
                <a href="../chapter-6/python_dependency_parsing.html">
            
                    
                    Python에서 Dependency Parsing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.3" data-path="../chapter-6/training_dependency_parsing.html">
            
                <a href="../chapter-6/training_dependency_parsing.html">
            
                    
                    Dependency Parsing 학습시키기
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 7 - Topic Models</li>
        
        
    
        <li class="chapter " data-level="8.1" data-path="../chapter-7/topic_models.html">
            
                <a href="../chapter-7/topic_models.html">
            
                    
                    Topic Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.2" data-path="../chapter-7/lda.html">
            
                <a href="../chapter-7/lda.html">
            
                    
                    Latent Dirichlet Allocation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.3" data-path="../chapter-7/lsa.html">
            
                <a href="../chapter-7/lsa.html">
            
                    
                    Latent Semantic Analysis
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.4" data-path="../chapter-7/hdp.html">
            
                <a href="../chapter-7/hdp.html">
            
                    
                    Hierarchical Dirichlet Process
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.5" data-path="../chapter-7/dtm.html">
            
                <a href="../chapter-7/dtm.html">
            
                    
                    Dynamic Topic Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.6" data-path="../chapter-7/scikit_topic_models.html">
            
                <a href="../chapter-7/scikit_topic_models.html">
            
                    
                    scikit-learn에서 topic models
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chpater 8 - Advanced Topic Modeling</li>
        
        
    
        <li class="chapter " data-level="9.1" data-path="../chapter-8/training_tips.html">
            
                <a href="../chapter-8/training_tips.html">
            
                    
                    Training Tips
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="9.2" data-path="../chapter-8/exploring_document.html">
            
                <a href="../chapter-8/exploring_document.html">
            
                    
                    Document 살펴보기
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="9.3" data-path="../chapter-8/evaluate_topic_models.html">
            
                <a href="../chapter-8/evaluate_topic_models.html">
            
                    
                    Topic Model 평가
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="9.4" data-path="../chapter-8/visualize.html">
            
                <a href="../chapter-8/visualize.html">
            
                    
                    Topic Model 시각화
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 9 - Cluster & Classify Text</li>
        
        
    
        <li class="chapter " data-level="10.1" data-path="../chapter-9/text_cluster.html">
            
                <a href="../chapter-9/text_cluster.html">
            
                    
                    텍스트 Clustering
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="10.2" data-path="../chapter-9/k_means.html">
            
                <a href="../chapter-9/k_means.html">
            
                    
                    K-means
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="10.3" data-path="../chapter-9/hierarchical.html">
            
                <a href="../chapter-9/hierarchical.html">
            
                    
                    계층적 Clustering
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="10.4" data-path="../chapter-9/text_classify.html">
            
                <a href="../chapter-9/text_classify.html">
            
                    
                    텍스트 Classifying
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 10 - Similarity Queries and Summarization</li>
        
        
    
        <li class="chapter " data-level="11.1" data-path="../chapter-10/similarity_metrics.html">
            
                <a href="../chapter-10/similarity_metrics.html">
            
                    
                    유사도 행렬
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="11.2" data-path="../chapter-10/similarity_queries.html">
            
                <a href="../chapter-10/similarity_queries.html">
            
                    
                    유사도 쿼리
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="11.3" data-path="../chapter-10/summarize.html">
            
                <a href="../chapter-10/summarize.html">
            
                    
                    텍스트 요약
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 11 - Vector Presentation</li>
        
        
    
        <li class="chapter " data-level="12.1" data-path="../chapter-11/word2vec.html">
            
                <a href="../chapter-11/word2vec.html">
            
                    
                    Word2Vec
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="12.2" data-path="../chapter-11/doc2vec.html">
            
                <a href="../chapter-11/doc2vec.html">
            
                    
                    Doc2Vec
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="12.3" data-path="../chapter-11/glove.html">
            
                <a href="../chapter-11/glove.html">
            
                    
                    GloVe
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="12.4" data-path="../chapter-11/etc.html">
            
                <a href="../chapter-11/etc.html">
            
                    
                    기타
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 12 - Deep Learning for Text</li>
        
        
    
        <li class="chapter " data-level="13.1" data-path="../chapter-12/dl4text.html">
            
                <a href="../chapter-12/dl4text.html">
            
                    
                    텍스트를 위한 딥러닝
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="13.2" data-path="../chapter-12/generate.html">
            
                <a href="../chapter-12/generate.html">
            
                    
                    텍스트 생성
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 13 - Deep Learning for Text Advanced</li>
        
        
    
        <li class="chapter " data-level="14.1" data-path="keras.html">
            
                <a href="keras.html">
            
                    
                    Keras와 딥러닝
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="14.2" data-path="spaCy.html">
            
                <a href="spaCy.html">
            
                    
                    spaCy와 딥러닝
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >spaCy와 딥러닝</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="spacy&#xC640;-&#xB525;&#xB7EC;&#xB2DD;">spaCy&#xC640; &#xB525;&#xB7EC;&#xB2DD;</h2>
<hr>
<pre><code>python -m spacy download en_vectors_web_lg
pip install cytoolz
</code></pre><p>&#xBA3C;&#xC800; &#xC124;&#xCE58;&#xD574;&#xC57C; &#xD560; &#xD30C;&#xC77C;&#xC774; &#xBA87; &#xAC1C; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.<br>&#xB2E4;&#xC74C; &#xBA85;&#xB839;&#xC5B4;&#xB97C; &#xC2E4;&#xD589;&#xD574;&#xC11C; &#xD30C;&#xC77C;&#xB4E4;&#xC744; &#xC124;&#xCE58;&#xD574; &#xBD05;&#xB2C8;&#xB2E4;.   </p>
<pre><code>&quot;&quot;&quot;
This example shows how to use an LSTM sentiment classification model trained
using Keras in spaCy. spaCy splits the document into sentences, and each
sentence is classified using the LSTM. The scores for the sentences are then
aggregated to give the document score. This kind of hierarchical model is quite
difficult in &quot;pure&quot; Keras or Tensorflow, but it&apos;s very effective. The Keras
example on this dataset performs quite poorly, because it cuts off the documents
so that they&apos;re a fixed size. This hurts review accuracy a lot, because people
often summarise their rating in the final sentence

Prerequisites:
spacy download en_vectors_web_lg
pip install keras==2.0.9

Compatible with: spaCy v2.0.0+
&quot;&quot;&quot;

import plac
import random
import pathlib
import cytoolz
import numpy
from keras.models import Sequential, model_from_json
from keras.layers import LSTM, Dense, Embedding, Bidirectional
from keras.layers import TimeDistributed
from keras.optimizers import Adam
import thinc.extra.datasets
from spacy.compat import pickle
import spacy


class SentimentAnalyser(object):
    @classmethod
    def load(cls, path, nlp, max_length=100):
        with (path / &quot;config.json&quot;).open() as file_:
            model = model_from_json(file_.read())
        with (path / &quot;model&quot;).open(&quot;rb&quot;) as file_:
            lstm_weights = pickle.load(file_)
        embeddings = get_embeddings(nlp.vocab)
        model.set_weights([embeddings] + lstm_weights)
        return cls(model, max_length=max_length)

    def __init__(self, model, max_length=100):
        self._model = model
        self.max_length = max_length

    def __call__(self, doc):
        X = get_features([doc], self.max_length)
        y = self._model.predict(X)
        self.set_sentiment(doc, y)

    def pipe(self, docs, batch_size=1000):
        for minibatch in cytoolz.partition_all(batch_size, docs):
            minibatch = list(minibatch)
            sentences = []
            for doc in minibatch:
                sentences.extend(doc.sents)
            Xs = get_features(sentences, self.max_length)
            ys = self._model.predict(Xs)
            for sent, label in zip(sentences, ys):
                sent.doc.sentiment += label - 0.5
            for doc in minibatch:
                yield doc

    def set_sentiment(self, doc, y):
        doc.sentiment = float(y[0])
        # Sentiment has a native slot for a single float.
        # For arbitrary data storage, there&apos;s:
        # doc.user_data[&apos;my_data&apos;] = y


def get_labelled_sentences(docs, doc_labels):
    labels = []
    sentences = []
    for doc, y in zip(docs, doc_labels):
        for sent in doc.sents:
            sentences.append(sent)
            labels.append(y)
    return sentences, numpy.asarray(labels, dtype=&quot;int32&quot;)


def get_features(docs, max_length):
    docs = list(docs)
    Xs = numpy.zeros((len(docs), max_length), dtype=&quot;int32&quot;)
    for i, doc in enumerate(docs):
        j = 0
        for token in doc:
            vector_id = token.vocab.vectors.find(key=token.orth)
            if vector_id &gt;= 0:
                Xs[i, j] = vector_id
            else:
                Xs[i, j] = 0
            j += 1
            if j &gt;= max_length:
                break
    return Xs


def train(
    train_texts,
    train_labels,
    dev_texts,
    dev_labels,
    lstm_shape,
    lstm_settings,
    lstm_optimizer,
    batch_size=100,
    nb_epoch=5,
    by_sentence=True,
):

    print(&quot;Loading spaCy&quot;)
    nlp = spacy.load(&quot;en_vectors_web_lg&quot;)
    nlp.add_pipe(nlp.create_pipe(&quot;sentencizer&quot;))
    embeddings = get_embeddings(nlp.vocab)
    model = compile_lstm(embeddings, lstm_shape, lstm_settings)

    print(&quot;Parsing texts...&quot;)
    train_docs = list(nlp.pipe(train_texts))
    dev_docs = list(nlp.pipe(dev_texts))
    if by_sentence:
        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)
        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)

    train_X = get_features(train_docs, lstm_shape[&quot;max_length&quot;])
    dev_X = get_features(dev_docs, lstm_shape[&quot;max_length&quot;])
    model.fit(
        train_X,
        train_labels,
        validation_data=(dev_X, dev_labels),
        epochs=nb_epoch,
        batch_size=batch_size,
    )
    return model


def compile_lstm(embeddings, shape, settings):
    model = Sequential()
    model.add(
        Embedding(
            embeddings.shape[0],
            embeddings.shape[1],
            input_length=shape[&quot;max_length&quot;],
            trainable=False,
            weights=[embeddings],
            mask_zero=True,
        )
    )
    model.add(TimeDistributed(Dense(shape[&quot;nr_hidden&quot;], use_bias=False)))
    model.add(
        Bidirectional(
            LSTM(
                shape[&quot;nr_hidden&quot;],
                recurrent_dropout=settings[&quot;dropout&quot;],
                dropout=settings[&quot;dropout&quot;],
            )
        )
    )
    model.add(Dense(shape[&quot;nr_class&quot;], activation=&quot;sigmoid&quot;))
    model.compile(
        optimizer=Adam(lr=settings[&quot;lr&quot;]),
        loss=&quot;binary_crossentropy&quot;,
        metrics=[&quot;accuracy&quot;],
    )
    return model


def get_embeddings(vocab):
    return vocab.vectors.data


def evaluate(model_dir, texts, labels, max_length=100):
    nlp = spacy.load(&quot;en_vectors_web_lg&quot;)
    nlp.add_pipe(nlp.create_pipe(&quot;sentencizer&quot;))
    nlp.add_pipe(SentimentAnalyser.load(model_dir, nlp, max_length=max_length))

    correct = 0
    i = 0
    for doc in nlp.pipe(texts, batch_size=1000):
        correct += bool(doc.sentiment &gt;= 0.5) == bool(labels[i])
        i += 1
    return float(correct) / i


def read_data(data_dir, limit=0):
    examples = []
    for subdir, label in ((&quot;pos&quot;, 1), (&quot;neg&quot;, 0)):
        for filename in (data_dir / subdir).iterdir():
            with filename.open() as file_:
                text = file_.read()
            examples.append((text, label))
    random.shuffle(examples)
    if limit &gt;= 1:
        examples = examples[:limit]
    return zip(*examples)  # Unzips into two lists


@plac.annotations(
    train_dir=(&quot;Location of training file or directory&quot;),
    dev_dir=(&quot;Location of development file or directory&quot;),
    model_dir=(&quot;Location of output model directory&quot;,),
    is_runtime=(&quot;Demonstrate run-time usage&quot;, &quot;flag&quot;, &quot;r&quot;, bool),
    nr_hidden=(&quot;Number of hidden units&quot;, &quot;option&quot;, &quot;H&quot;, int),
    max_length=(&quot;Maximum sentence length&quot;, &quot;option&quot;, &quot;L&quot;, int),
    dropout=(&quot;Dropout&quot;, &quot;option&quot;, &quot;d&quot;, float),
    learn_rate=(&quot;Learn rate&quot;, &quot;option&quot;, &quot;e&quot;, float),
    nb_epoch=(&quot;Number of training epochs&quot;, &quot;option&quot;, &quot;i&quot;, int),
    batch_size=(&quot;Size of minibatches for training LSTM&quot;, &quot;option&quot;, &quot;b&quot;, int),
    nr_examples=(&quot;Limit to N examples&quot;, &quot;option&quot;, &quot;n&quot;, int),
)
def main(
    model_dir=None,
    train_dir=None,
    dev_dir=None,
    is_runtime=False,
    nr_hidden=64,
    max_length=100,  # Shape
    dropout=0.5,
    learn_rate=0.001,  # General NN config
    nb_epoch=5,
    batch_size=256,
    nr_examples=-1,
):  # Training params
    if model_dir is not None:
        model_dir = pathlib.Path(model_dir)
    if train_dir is None or dev_dir is None:
        imdb_data = thinc.extra.datasets.imdb()
    if is_runtime:
        if dev_dir is None:
            dev_texts, dev_labels = zip(*imdb_data[1])
        else:
            dev_texts, dev_labels = read_data(dev_dir)
        acc = evaluate(model_dir, dev_texts, dev_labels, max_length=max_length)
        print(acc)
    else:
        if train_dir is None:
            train_texts, train_labels = zip(*imdb_data[0])
        else:
            print(&quot;Read data&quot;)
            train_texts, train_labels = read_data(train_dir, limit=nr_examples)
        if dev_dir is None:
            dev_texts, dev_labels = zip(*imdb_data[1])
        else:
            dev_texts, dev_labels = read_data(dev_dir, imdb_data, limit=nr_examples)
        train_labels = numpy.asarray(train_labels, dtype=&quot;int32&quot;)
        dev_labels = numpy.asarray(dev_labels, dtype=&quot;int32&quot;)
        lstm = train(
            train_texts,
            train_labels,
            dev_texts,
            dev_labels,
            {&quot;nr_hidden&quot;: nr_hidden, &quot;max_length&quot;: max_length, &quot;nr_class&quot;: 1},
            {&quot;dropout&quot;: dropout, &quot;lr&quot;: learn_rate},
            {},
            nb_epoch=nb_epoch,
            batch_size=batch_size,
        )
        weights = lstm.get_weights()
        if model_dir is not None:
            with (model_dir / &quot;model&quot;).open(&quot;wb&quot;) as file_:
                pickle.dump(weights[1:], file_)
            with (model_dir / &quot;config.json&quot;).open(&quot;w&quot;) as file_:
                file_.write(lstm.to_json())


if __name__ == &quot;__main__&quot;:
    plac.call(main)
</code></pre><p>&#xC608;&#xC81C;&#xB97C; &#xC2E4;&#xD589;&#xD558;&#xBA74; accuracy 72%&#xC815;&#xB3C4;&#xB85C; &#xB098;&#xC635;&#xB2C8;&#xB2E4;.   </p>
<p>&#xCF54;&#xB4DC;&#xB97C; &#xC0B4;&#xD3B4;&#xBCF4;&#xBA74;, spaCy&#xB294; &#xD14D;&#xC2A4;&#xD2B8;&#xB97C; &#xC815;&#xC81C;&#xD558;&#xB294; &#xC5ED;&#xD560;&#xB9CC; &#xD558;&#xACE0; &#xC2E4;&#xC9C8;&#xC801;&#xC778; deep learning&#xC740; keras&#xC5D0;&#xC11C; &#xD558;&#xB294; &#xAC83;&#xC744; &#xC54C; &#xC218; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.<br>Keras Deep Learning &#xBAA8;&#xB378;&#xC740; Bidirectional LSTM&#xC744; &#xC774;&#xC6A9;&#xD574;&#xC11C; &#xAD6C;&#xC131;&#xD588;&#xC2B5;&#xB2C8;&#xB2E4;.<br>&#xBCF5;&#xC7A1;&#xD574; &#xBCF4;&#xC774;&#xC9C0;&#xB9CC;, &#xC774; &#xC608;&#xC81C;&#xB294; spaCy&#xC640; Keras&#xB97C; &#xC774;&#xC6A9;&#xD558;&#xC5EC; Text Classification&#xC744; &#xC218;&#xD589;&#xD558;&#xB294; &#xC88B;&#xC740; &#xC608;&#xC81C;&#xC785;&#xB2C8;&#xB2E4;.   </p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="keras.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page: Keras와 딥러닝">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"spaCy와 딥러닝","level":"14.2","depth":1,"previous":{"title":"Keras와 딥러닝","level":"14.1","depth":1,"path":"chapter-13/keras.md","ref":"chapter-13/keras.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"chapter-13/spaCy.md","mtime":"2020-09-16T09:32:54.110Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-09-16T09:32:58.674Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>



<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Keras와 딥러닝 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="spaCy.html" />
    
    
    <link rel="prev" href="../chapter-12/generate.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    프로젝트 소개
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 1 - Text Analysis</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../chapter-1/what_is_text_analysis.html">
            
                <a href="../chapter-1/what_is_text_analysis.html">
            
                    
                    Text Analysis란?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../chapter-1/importance_of_input.html">
            
                <a href="../chapter-1/importance_of_input.html">
            
                    
                    Garbage in, Garbage out
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../chapter-1/importance_of_text_analysis.html">
            
                <a href="../chapter-1/importance_of_text_analysis.html">
            
                    
                    Text Analysis를 시작하자!
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="../chapter-1/why_python.html">
            
                <a href="../chapter-1/why_python.html">
            
                    
                    Python?
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 2 - Language Model</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../chatper-2/spaCy.md">
            
                <span>
            
                    
                    spaCy
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../chapter-2/install_lm.html">
            
                <a href="../chapter-2/install_lm.html">
            
                    
                    Language Model 설치
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="../chapter-2/tokenize_text.html">
            
                <a href="../chapter-2/tokenize_text.html">
            
                    
                    텍스트 토큰화
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.4" data-path="../chapter-2/pos_tagging.html">
            
                <a href="../chapter-2/pos_tagging.html">
            
                    
                    POS 태깅
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="../chapter-2/ner_tagging.html">
            
                <a href="../chapter-2/ner_tagging.html">
            
                    
                    NER 태깅
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="../chapter-2/preprocessing.html">
            
                <a href="../chapter-2/preprocessing.html">
            
                    
                    텍스트 전처리
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 3 - Text를 Vector로</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="../chapter-3/gensim.html">
            
                <a href="../chapter-3/gensim.html">
            
                    
                    Gensim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="../chapter-3/bow.html">
            
                <a href="../chapter-3/bow.html">
            
                    
                    Bag-of-words
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="../chapter-3/tf_idf.html">
            
                <a href="../chapter-3/tf_idf.html">
            
                    
                    TF-IDF
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="../chapter-3/vector_transformation_in_gensim.html">
            
                <a href="../chapter-3/vector_transformation_in_gensim.html">
            
                    
                    Gensim에서 벡터 표현법
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="../chapter-3/n_gram.html">
            
                <a href="../chapter-3/n_gram.html">
            
                    
                    N-gram과 전처리
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chpater 4 - POS Tagging</li>
        
        
    
        <li class="chapter " data-level="5.1" data-path="../chapter-4/pos_tagging.html">
            
                <a href="../chapter-4/pos_tagging.html">
            
                    
                    POS Tagging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.2" data-path="../chapter-4/python_pos_tagging.html">
            
                <a href="../chapter-4/python_pos_tagging.html">
            
                    
                    Python에서 POS Tagging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.3" data-path="../chapter-4/training_pos_tagger.html">
            
                <a href="../chapter-4/training_pos_tagger.html">
            
                    
                    POS Tagger 학습시키기
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.4" data-path="../chapter-4/pos_tagging_code.html">
            
                <a href="../chapter-4/pos_tagging_code.html">
            
                    
                    POS Tagging Code
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 5 - NER Tagging</li>
        
        
    
        <li class="chapter " data-level="6.1" data-path="../chapter-5/ner_tagging.html">
            
                <a href="../chapter-5/ner_tagging.html">
            
                    
                    NER Tagging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.2" data-path="../chapter-5/python_ner_tagging.html">
            
                <a href="../chapter-5/python_ner_tagging.html">
            
                    
                    Python에서 NER Tagging
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.3" data-path="../chapter-5/training_ner_tagger.html">
            
                <a href="../chapter-5/training_ner_tagger.html">
            
                    
                    NER Tagger 학습시키기
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.4" data-path="../chapter-5/ner_tagging_code.html">
            
                <a href="../chapter-5/ner_tagging_code.html">
            
                    
                    NER Tagging Code
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 6 - Dependency Parsing</li>
        
        
    
        <li class="chapter " data-level="7.1" data-path="../chapter-6/dependency_parsing.html">
            
                <a href="../chapter-6/dependency_parsing.html">
            
                    
                    Dependency Parsing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.2" data-path="../chapter-6/python_dependency_parsing.html">
            
                <a href="../chapter-6/python_dependency_parsing.html">
            
                    
                    Python에서 Dependency Parsing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.3" data-path="../chapter-6/training_dependency_parsing.html">
            
                <a href="../chapter-6/training_dependency_parsing.html">
            
                    
                    Dependency Parsing 학습시키기
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 7 - Topic Models</li>
        
        
    
        <li class="chapter " data-level="8.1" data-path="../chapter-7/topic_models.html">
            
                <a href="../chapter-7/topic_models.html">
            
                    
                    Topic Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.2" data-path="../chapter-7/lda.html">
            
                <a href="../chapter-7/lda.html">
            
                    
                    Latent Dirichlet Allocation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.3" data-path="../chapter-7/lsa.html">
            
                <a href="../chapter-7/lsa.html">
            
                    
                    Latent Semantic Analysis
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.4" data-path="../chapter-7/hdp.html">
            
                <a href="../chapter-7/hdp.html">
            
                    
                    Hierarchical Dirichlet Process
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.5" data-path="../chapter-7/dtm.html">
            
                <a href="../chapter-7/dtm.html">
            
                    
                    Dynamic Topic Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.6" data-path="../chapter-7/scikit_topic_models.html">
            
                <a href="../chapter-7/scikit_topic_models.html">
            
                    
                    scikit-learn에서 topic models
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chpater 8 - Advanced Topic Modeling</li>
        
        
    
        <li class="chapter " data-level="9.1" data-path="../chapter-8/training_tips.html">
            
                <a href="../chapter-8/training_tips.html">
            
                    
                    Training Tips
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="9.2" data-path="../chapter-8/exploring_document.html">
            
                <a href="../chapter-8/exploring_document.html">
            
                    
                    Document 살펴보기
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="9.3" data-path="../chapter-8/evaluate_topic_models.html">
            
                <a href="../chapter-8/evaluate_topic_models.html">
            
                    
                    Topic Model 평가
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="9.4" data-path="../chapter-8/visualize.html">
            
                <a href="../chapter-8/visualize.html">
            
                    
                    Topic Model 시각화
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 9 - Cluster & Classify Text</li>
        
        
    
        <li class="chapter " data-level="10.1" data-path="../chapter-9/text_cluster.html">
            
                <a href="../chapter-9/text_cluster.html">
            
                    
                    텍스트 Clustering
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="10.2" data-path="../chapter-9/k_means.html">
            
                <a href="../chapter-9/k_means.html">
            
                    
                    K-means
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="10.3" data-path="../chapter-9/hierarchical.html">
            
                <a href="../chapter-9/hierarchical.html">
            
                    
                    계층적 Clustering
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="10.4" data-path="../chapter-9/text_classify.html">
            
                <a href="../chapter-9/text_classify.html">
            
                    
                    텍스트 Classifying
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 10 - Similarity Queries and Summarization</li>
        
        
    
        <li class="chapter " data-level="11.1" data-path="../chapter-10/similarity_metrics.html">
            
                <a href="../chapter-10/similarity_metrics.html">
            
                    
                    유사도 행렬
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="11.2" data-path="../chapter-10/similarity_queries.html">
            
                <a href="../chapter-10/similarity_queries.html">
            
                    
                    유사도 쿼리
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="11.3" data-path="../chapter-10/summarize.html">
            
                <a href="../chapter-10/summarize.html">
            
                    
                    텍스트 요약
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 11 - Vector Presentation</li>
        
        
    
        <li class="chapter " data-level="12.1" data-path="../chapter-11/word2vec.html">
            
                <a href="../chapter-11/word2vec.html">
            
                    
                    Word2Vec
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="12.2" data-path="../chapter-11/doc2vec.html">
            
                <a href="../chapter-11/doc2vec.html">
            
                    
                    Doc2Vec
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="12.3" data-path="../chapter-11/glove.html">
            
                <a href="../chapter-11/glove.html">
            
                    
                    GloVe
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="12.4" data-path="../chapter-11/etc.html">
            
                <a href="../chapter-11/etc.html">
            
                    
                    기타
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 12 - Deep Learning for Text</li>
        
        
    
        <li class="chapter " data-level="13.1" data-path="../chapter-12/dl4text.html">
            
                <a href="../chapter-12/dl4text.html">
            
                    
                    텍스트를 위한 딥러닝
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="13.2" data-path="../chapter-12/generate.html">
            
                <a href="../chapter-12/generate.html">
            
                    
                    텍스트 생성
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 13 - Deep Learning for Text Advanced</li>
        
        
    
        <li class="chapter active" data-level="14.1" data-path="keras.html">
            
                <a href="keras.html">
            
                    
                    Keras와 딥러닝
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="14.2" data-path="spaCy.html">
            
                <a href="spaCy.html">
            
                    
                    spaCy와 딥러닝
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Keras와 딥러닝</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="keras&#xC640;-&#xB525;&#xB7EC;&#xB2DD;">Keras&#xC640; &#xB525;&#xB7EC;&#xB2DD;</h2>
<hr>
<h3 id="rnn&#xC744;-&#xC774;&#xC6A9;&#xD55C;-classification">RNN&#xC744; &#xC774;&#xC6A9;&#xD55C; Classification</h3>
<p>&#xC9C0;&#xB09C; &#xC7A5;&#xC5D0;&#xC11C;&#xB294; Keras&#xB97C; &#xC774;&#xC6A9;&#xD574;&#xC11C; &#xD14D;&#xC2A4;&#xD2B8;&#xB97C; &#xC0DD;&#xC131;&#xD558;&#xB294; &#xB525; &#xB7EC;&#xB2DD; &#xBAA8;&#xB378;&#xC744; &#xAD6C;&#xCD95;&#xD574;&#xC11C; &#xC9C1;&#xC811; &#xD559;&#xC2B5;&#xD558;&#xACE0;, &#xD14C;&#xC2A4;&#xD2B8;&#xAE4C;&#xC9C0; &#xD574; &#xBCF4;&#xC558;&#xC2B5;&#xB2C8;&#xB2E4;.<br>&#xC774;&#xBC88;&#xC5D0;&#xB294; Keras&#xB97C; &#xC774;&#xC6A9;&#xD574;&#xC11C; classification &#xACFC;&#xC81C;&#xB97C; &#xC218;&#xD589;&#xD574; &#xBCF4;&#xACA0;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<pre><code>from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from keras.datasets import imdb

max_features = 20000
maxlen = 80
batch_size = 32
print(&apos;Loading data...&apos;)

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
print(len(x_train), &apos;train sequences&apos;)
print(len(x_test), &apos;test sequences&apos;)

x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print(&apos;x_train shape: &apos;, x_train.shape)
print(&apos;x_test shape: &apos;, x_test.shape)

print(&apos;Build model...&apos;)
model = Sequential()
model.add(Embedding(max_features, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation=&apos;sigmoid&apos;))

model.compile(loss=&apos;binary_crossentropy&apos;, optimizer=&apos;adam&apos;, metrics=[&apos;accuracy&apos;])

print(&apos;Train...&apos;)
model.fit(x_train, y_train, batch_size=batch_size, epochs=15, validation_data=(x_test, y_test))

score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)

print(&apos;Test score: &apos;, score)
print(&apos;Test accuracy: &apos;, acc)
</code></pre><p>&#xC774;&#xBC88; &#xCF54;&#xB4DC;&#xC758; &#xB370;&#xC774;&#xD130;&#xB294; Keras&#xC5D0; &#xC774;&#xBBF8; &#xB0B4;&#xC7A5;&#xB418;&#xC5B4; &#xC788;&#xB294; imdb &#xB370;&#xC774;&#xD130;&#xB85C; &#xC9C4;&#xD589;&#xD588;&#xC2B5;&#xB2C8;&#xB2E4;.<br>imdb &#xB370;&#xC774;&#xD130;&#xB294; &#xC774;&#xBBF8; preprocessing&#xC774; &#xC9C4;&#xD589;&#xB418;&#xC5B4; &#xC788;&#xAE30; &#xB54C;&#xBB38;&#xC5D0; &#xB530;&#xB85C; preprocessing&#xC744; &#xD560; &#xD544;&#xC694;&#xB294; &#xC5C6;&#xC2B5;&#xB2C8;&#xB2E4;.<br>imdb &#xB370;&#xC774;&#xD130;&#xB294; &#xB77C;&#xBCA8;&#xB85C; &#xBD84;&#xB958;&#xAC00; &#xB418;&#xC5B4; &#xC788;&#xC5B4;&#xC11C; &#xD3B8;&#xD558;&#xAC8C; &#xAC00;&#xC838;&#xB2E4; &#xC4F0;&#xAE30;&#xB9CC; &#xD558;&#xBA74; &#xB429;&#xB2C8;&#xB2E4;.   </p>
<p>&#xC774;&#xBC88; &#xC608;&#xC81C;&#xC5D0;&#xC11C; &#xB124;&#xD2B8;&#xC6CC;&#xD06C;&#xB294; Embedding Layer&#xC640; LSTM Layer, Sigmoid Layer&#xB85C; &#xAD6C;&#xC131;&#xB418;&#xC5B4; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.<br>Embedding Layer&#xB294; 20000&#xAC1C;&#xC758; &#xB2E8;&#xC5B4;&#xB85C; &#xB418;&#xC5B4; &#xC788;&#xB294; &#xC785;&#xB825;&#xC744; 128&#xAC1C;&#xC758; &#xCC28;&#xC6D0;&#xC73C;&#xB85C; &#xC904;&#xC77C; &#xAC83;&#xC785;&#xB2C8;&#xB2E4;.<br>LSTM Layer&#xB294; &#xC2E4;&#xC81C;&#xB85C; &#xBB38;&#xB9E5;&#xC744; &#xAC00;&#xC9C0;&#xACE0; &#xC2E0;&#xACBD;&#xB9DD;&#xC744; &#xAD6C;&#xC131;&#xD558;&#xAC8C; &#xB429;&#xB2C8;&#xB2E4;.<br>Sigmoid Layer&#xC758; &#xACBD;&#xC6B0; LSTM Layer&#xC758; &#xCD9C;&#xB825;&#xC744; &#xAC00;&#xC9C0;&#xACE0; &#xC2E4;&#xC81C;&#xB85C; &#xBD84;&#xB958;&#xB97C; &#xD558;&#xB294; &#xC791;&#xC5C5;&#xC744; &#xAC70;&#xCE58;&#xAC8C; &#xB429;&#xB2C8;&#xB2E4;.   </p>
<p>Model&#xC744; adam optimizer&#xB97C; &#xC0AC;&#xC6A9;&#xD574;&#xC11C; &#xAD6C;&#xC131;&#xD588;&#xACE0;, &#xD559;&#xC2B5; &#xD6C4;&#xC5D0; &#xC2E4;&#xC81C;&#xB85C; &#xD14C;&#xC2A4;&#xD2B8;&#xB97C; &#xC9C4;&#xD589;&#xD574; &#xBCF4;&#xC558;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<p>&#xCF54;&#xB4DC;&#xB97C; &#xC2E4;&#xD589;&#xD558;&#xBA74; &#xAF64; &#xAE34; &#xC2DC;&#xAC04;&#xC758; &#xD559;&#xC2B5; &#xD6C4;&#xC5D0; &#xACB0;&#xACFC;&#xB97C; &#xC5BB;&#xC744; &#xC218; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<pre><code>Test score:  1.087781548500061
Test accuracy:  0.8176800012588501
</code></pre><p>&#xACB0;&#xACFC;&#xB294; &#xB2E4;&#xC74C;&#xACFC; &#xAC19;&#xC774; &#xB098;&#xC635;&#xB2C8;&#xB2E4;.<br>&#xC815;&#xD655;&#xB3C4;&#xAC00; 0.81 &#xC815;&#xB3C4;&#xBA74; &#xADF8;&#xB807;&#xAC8C; &#xB098;&#xC058;&#xC9C0;&#xB294; &#xC54A;&#xC9C0;&#xB9CC; &#xD5A5;&#xC0C1;&#xB420; &#xC5EC;&#xC9C0;&#xAC00; &#xB9CE;&#xC740; &#xAC83; &#xAC19;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<h3 id="cnn&#xC744;-&#xC774;&#xC6A9;&#xD55C;-classification">CNN&#xC744; &#xC774;&#xC6A9;&#xD55C; Classification</h3>
<p>CNN&#xC744; &#xC774;&#xC6A9;&#xD574;&#xC11C; &#xC9C4;&#xD589;&#xD558;&#xB294; &#xC608;&#xC2DC;&#xB3C4; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<pre><code>from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import LSTM, Embedding
from keras.layers import Conv1D, MaxPooling1D
from keras.datasets import imdb

kernel_size = 5
filters = 64
pool_size = 4

max_features = 20000
maxlen = 100
embedding_size = 128

lstm_output_size = 70

batch_size = 30
epochs = 10
print(&apos;Loading data...&apos;)

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
print(len(x_train), &apos;train sequences&apos;)
print(len(x_test), &apos;test sequences&apos;)

x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print(&apos;x_train shape: &apos;, x_train.shape)
print(&apos;x_test shape: &apos;, x_test.shape)

print(&apos;Build model...&apos;)
model = Sequential()
model.add(Embedding(max_features, embedding_size, input_length=maxlen))
model.add(Dropout(0.25))
model.add(Conv1D(filters, kernel_size, padding=&apos;valid&apos;, activation=&apos;relu&apos;, strides=1))
model.add(MaxPooling1D(pool_size=pool_size))
model.add(LSTM(lstm_output_size))
model.add(Dense(1))
model.add(Activation(&apos;sigmoid&apos;))

model.compile(loss=&apos;binary_crossentropy&apos;, optimizer=&apos;adam&apos;, metrics=[&apos;accuracy&apos;])

print(&apos;Train...&apos;)
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))

score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)

print(&apos;Test score: &apos;, score)
print(&apos;Test accuracy: &apos;, acc)
</code></pre><p>Parameter&#xB97C; &#xC124;&#xC815;&#xD558;&#xB294; &#xBD80;&#xBD84;&#xC774; &#xC870;&#xAE08; &#xB354; &#xAE38;&#xACE0;, &#xBAA8;&#xB378;&#xC774; &#xC870;&#xAE08; &#xB354; &#xBCF5;&#xC7A1;&#xD55C; &#xAC83; &#xC678;&#xC5D0;&#xB294; &#xBE44;&#xC2B7;&#xD569;&#xB2C8;&#xB2E4;.<br>&#xBAA8;&#xB378;&#xC744; &#xC0B4;&#xD3B4;&#xBCF4;&#xBA74;, Embedding&#xC73C;&#xB85C; &#xCC28;&#xC6D0;&#xC744; &#xCD95;&#xC18C;&#xD558;&#xACE0;, Dropout&#xC73C;&#xB85C; Overfitting&#xC744; &#xBC29;&#xC9C0;&#xD569;&#xB2C8;&#xB2E4;.<br>&#xADF8; &#xD6C4;, Conv1D &#xB808;&#xC774;&#xC5B4;&#xB97C; &#xD1B5;&#xD574; 1&#xCC28;&#xC6D0; &#xC0C1;&#xC5D0;&#xC11C; 5&#xAC1C; &#xB2E8;&#xC704;&#xB85C; &#xC798;&#xB77C; &#xBCF4;&#xBA74;&#xC11C; &#xD2B9;&#xC9D5;&#xC744; &#xCD94;&#xCD9C;&#xD558;&#xACE0;, MaxPooling1D&#xB97C; &#xD1B5;&#xD574; &#xADF8; layer&#xB4E4;&#xC758; &#xCD5C;&#xB300;&#xCE58;&#xB97C; &#xC885;&#xD569;&#xD569;&#xB2C8;&#xB2E4;.<br>LSTM&#xC73C;&#xB85C; &#xBB38;&#xB9E5; &#xC774;&#xD574;&#xB97C; &#xD55C; &#xD6C4;, Dense Layer&#xB97C; &#xAC70;&#xCCD0; Activation Layer&#xB85C; &#xAC00;&#xAC8C; &#xB429;&#xB2C8;&#xB2E4;.   </p>
<p>&#xACB0;&#xACFC;&#xB294; &#xB2E4;&#xC74C;&#xACFC; &#xAC19;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<pre><code>Test score:  0.35533609986305237
Test accuracy:  0.8527200222015381
</code></pre><p>&#xB2E8;&#xC21C;&#xD788; RNN&#xB9CC; &#xC801;&#xC6A9;&#xD55C; &#xAC83;&#xBCF4;&#xB2E4; &#xB2E4;&#xC591;&#xD55C; Layer&#xB85C; &#xBCF5;&#xC7A1;&#xD558;&#xAC8C; &#xAD6C;&#xC131;&#xD55C; CNN Network&#xAC00; &#xC720;&#xC758;&#xBBF8;&#xD55C; &#xC815;&#xD655;&#xB3C4; &#xC0C1;&#xC2B9;&#xC744; &#xC774;&#xB904;&#xB0C8;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<h3 id="word-embedding-&#xC0AC;&#xC6A9;&#xD558;&#xAE30;">Word Embedding &#xC0AC;&#xC6A9;&#xD558;&#xAE30;</h3>
<p>&#xC774;&#xBC88;&#xC5D0;&#xB294; Word Embedding&#xC744; &#xC774;&#xC6A9;&#xD574;&#xC11C; &#xB354; &#xC815;&#xD655;&#xB3C4;&#xB97C; &#xB192;&#xC778; &#xBAA8;&#xB378;&#xC744; &#xB9CC;&#xB4E4;&#xC5B4; &#xBCF4;&#xACA0;&#xC2B5;&#xB2C8;&#xB2E4;.<br>&#xC5EC;&#xAE30;&#xC5D0; &#xC0AC;&#xC6A9;&#xB418;&#xB294; glove data&#xB294; <a href="http://nlp.stanford.edu/data/glove.6B.zip" target="_blank">glove</a>&#xC5D0;&#xC11C; &#xB2E4;&#xC6B4;&#xB85C;&#xB4DC; &#xBC1B;&#xC744; &#xC218; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.<br>&#xC5EC;&#xAE30;&#xC5D0; &#xC0AC;&#xC6A9;&#xB418;&#xB294; newsgroup data&#xB294; <a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html" target="_blank">newsgroup</a>&#xC5D0;&#xC11C; &#xB2E4;&#xC6B4;&#xB85C;&#xB4DC; &#xBC1B;&#xC744; &#xC218; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<pre><code>from __future__ import print_function

import os
import sys
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import Dense, Input, GlobalMaxPooling1D
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
from keras.initializers import Constant


BASE_DIR = &apos;&apos;
GLOVE_DIR = &apos;&apos;
TEXT_DATA_DIR = os.path.join(BASE_DIR, &apos;20_newsgroup&apos;)
MAX_SEQUENCE_LENGTH = 1000
MAX_NUM_WORDS = 20000
EMBEDDING_DIM = 100
VALIDATION_SPLIT = 0.2

# first, build index mapping words in the embeddings set
# to their embedding vector

print(&apos;Indexing word vectors.&apos;)

embeddings_index = {}
with open(os.path.join(GLOVE_DIR, &apos;glove.6B.100d.txt&apos;)) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, &apos;f&apos;, sep=&apos; &apos;)
        embeddings_index[word] = coefs

print(&apos;Found %s word vectors.&apos; % len(embeddings_index))

# second, prepare text samples and their labels
print(&apos;Processing text dataset&apos;)

texts = []  # list of text samples
labels_index = {}  # dictionary mapping label name to numeric id
labels = []  # list of label ids
for name in sorted(os.listdir(TEXT_DATA_DIR)):
    path = os.path.join(TEXT_DATA_DIR, name)
    if os.path.isdir(path):
        label_id = len(labels_index)
        labels_index[name] = label_id
        for fname in sorted(os.listdir(path)):
            if fname.isdigit():
                fpath = os.path.join(path, fname)
                args = {} if sys.version_info &lt; (3,) else {&apos;encoding&apos;: &apos;latin-1&apos;}
                with open(fpath, **args) as f:
                    t = f.read()
                    i = t.find(&apos;\n\n&apos;)  # skip header
                    if 0 &lt; i:
                        t = t[i:]
                    texts.append(t)
                labels.append(label_id)

print(&apos;Found %s texts.&apos; % len(texts))

# finally, vectorize the text samples into a 2D integer tensor
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print(&apos;Found %s unique tokens.&apos; % len(word_index))

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

labels = to_categorical(np.asarray(labels))
print(&apos;Shape of data tensor:&apos;, data.shape)
print(&apos;Shape of label tensor:&apos;, labels.shape)

# split the data into a training set and a validation set
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-num_validation_samples]
y_train = labels[:-num_validation_samples]
x_val = data[-num_validation_samples:]
y_val = labels[-num_validation_samples:]

print(&apos;Preparing embedding matrix.&apos;)

# prepare embedding matrix
num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if i &gt;= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
embedding_layer = Embedding(num_words,
                            EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)

print(&apos;Training model.&apos;)

# train a 1D convnet with global maxpooling
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=&apos;int32&apos;)
embedded_sequences = embedding_layer(sequence_input)
x = Conv1D(128, 5, activation=&apos;relu&apos;)(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation=&apos;relu&apos;)(x)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation=&apos;relu&apos;)(x)
x = GlobalMaxPooling1D()(x)
x = Dense(128, activation=&apos;relu&apos;)(x)
preds = Dense(len(labels_index), activation=&apos;softmax&apos;)(x)

model = Model(sequence_input, preds)
model.compile(loss=&apos;categorical_crossentropy&apos;,
              optimizer=&apos;rmsprop&apos;,
              metrics=[&apos;acc&apos;])

model.fit(x_train, y_train,
          batch_size=128,
          epochs=10,
          validation_data=(x_val, y_val))

score, acc = model.evaluate(x_val, y_val, batch_size=128)

print(&apos;Test score: &apos;, score)
print(&apos;Test accuracy: &apos;, acc)
</code></pre><p>&#xACB0;&#xACFC;&#xB294; &#xB2E4;&#xC74C;&#xACFC; &#xAC19;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<pre><code>Test score:  0.9687448740005493
Test accuracy:  0.7211803197860718
</code></pre><p>&#xC544;&#xAE4C; &#xC804;&#xACFC;&#xB294; &#xB370;&#xC774;&#xD130;&#xC14B;&#xB3C4; &#xB2E4;&#xB974;&#xACE0;, &#xBAA8;&#xB378; &#xAD6C;&#xC131;&#xB3C4; &#xC870;&#xAE08; &#xB2E4;&#xB974;&#xAE30; &#xB54C;&#xBB38;&#xC5D0; &#xC624;&#xD788;&#xB824; &#xC774;&#xC804; &#xACB0;&#xACFC;&#xBCF4;&#xB2E4; &#xB354; &#xB0AE;&#xAC8C; &#xB098;&#xC62C; &#xC218; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.   </p>
<p>&#xC774;&#xCC98;&#xB7FC; Word Embedding &#xC911; &#xD558;&#xB098;&#xC778; GloVe&#xB97C; &#xC0AC;&#xC6A9;&#xD574;&#xC11C; &#xC720;&#xC758;&#xBBF8;&#xD55C; &#xC815;&#xD655;&#xB3C4; &#xC0C1;&#xC2B9;&#xC744; &#xC774;&#xB904;&#xB0BC; &#xC218; &#xC788;&#xC2B5;&#xB2C8;&#xB2E4;.<br>&#xACFC;&#xC815;&#xC740; &#xAC04;&#xB2E8;&#xD569;&#xB2C8;&#xB2E4;.<br>&#xBA3C;&#xC800; &#xC800;&#xC7A5;&#xB41C; GloVe &#xB370;&#xC774;&#xD130;&#xB97C; &#xBD88;&#xB7EC;&#xC640;&#xC11C; &#xBA54;&#xBAA8;&#xB9AC;&#xC5D0; &#xC801;&#xC7AC;&#xD569;&#xB2C8;&#xB2E4;.<br>dataset&#xC744; &#xD1A0;&#xD070;&#xD654;&#xD558;&#xB294; &#xB4F1;&#xC758; preprocessing&#xC744; &#xC9C4;&#xD589;&#xD55C; &#xD6C4;&#xC5D0; glove data&#xB97C; &#xC774;&#xC6A9;&#xD574;&#xC11C; embedding &#xCE35;&#xC744; &#xB9CC;&#xB4E4;&#xC5B4;&#xB0C5;&#xB2C8;&#xB2E4;.<br>&#xADF8; &#xD6C4;, Conv1D&#xC640; MaxPooling&#xC744; &#xC5EC;&#xB7EC;&#xAC1C; &#xC313;&#xC544;&#xC11C; &#xBAA8;&#xB378;&#xC744; &#xAD6C;&#xC131;&#xD569;&#xB2C8;&#xB2E4;.<br>&#xB9C8;&#xC9C0;&#xB9C9;&#xC73C;&#xB85C; &#xBAA8;&#xB378;&#xC744; &#xD6C8;&#xB828;&#xC2DC;&#xD0A4;&#xBA74; &#xC791;&#xC5C5;&#xC740; &#xB05D;&#xB0A9;&#xB2C8;&#xB2E4;.   </p>
<p>Word Embedding&#xC740; model&#xC5D0; &#xB300;&#xD55C; &#xB354; &#xB9CE;&#xC740; context&#xB97C; &#xD3EC;&#xD568;&#xD558;&#xACE0; &#xC788;&#xACE0;, &#xB2E8;&#xC5B4;&#xB97C; &#xB354; &#xC798; &#xC124;&#xBA85;&#xD560; &#xC218; &#xC788;&#xAE30; &#xB54C;&#xBB38;&#xC5D0;, &#xC131;&#xB2A5; &#xD5A5;&#xC0C1;&#xC774; &#xC77C;&#xC5B4;&#xB098;&#xB294; &#xAC83;&#xC740; &#xB2F9;&#xC5F0;&#xD569;&#xB2C8;&#xB2E4;.   </p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../chapter-12/generate.html" class="navigation navigation-prev " aria-label="Previous page: 텍스트 생성">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="spaCy.html" class="navigation navigation-next " aria-label="Next page: spaCy와 딥러닝">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Keras와 딥러닝","level":"14.1","depth":1,"next":{"title":"spaCy와 딥러닝","level":"14.2","depth":1,"path":"chapter-13/spaCy.md","ref":"chapter-13/spaCy.md","articles":[]},"previous":{"title":"텍스트 생성","level":"13.2","depth":1,"path":"chapter-12/generate.md","ref":"chapter-12/generate.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"chapter-13/keras.md","mtime":"2020-09-16T08:48:47.806Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-09-17T06:10:12.181Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

